{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch import Tensor\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = ['airplane', 'bus', 'cat',\n",
    "               'dolphin', 'guitar', 'hurricane',\n",
    "               'laptop', 'mountain', 'sheep',\n",
    "               'The_Eiffel_Tower', 'tree' , 'umbrella']\n",
    "\n",
    "labels_dict = {\n",
    "    'airplane': np.array([1,0,0,0,0,0,0,0,0,0,0,0]),\n",
    "    'bus' : np.array([0,1,0,0,0,0,0,0,0,0,0,0]),\n",
    "    'cat' : np.array([0,0,1,0,0,0,0,0,0,0,0,0]),\n",
    "    'dolphin' : np.array([0,0,0,1,0,0,0,0,0,0,0,0]),\n",
    "    'guitar' : np.array([0,0,0,0,1,0,0,0,0,0,0,0]),\n",
    "    'hurricane' : np.array([0,0,0,0,0,1,0,0,0,0,0,0]),\n",
    "    'laptop' : np.array([0,0,0,0,0,0,1,0,0,0,0,0]),\n",
    "    'mountain' : np.array([0,0,0,0,0,0,0,1,0,0,0,0]),\n",
    "    'sheep' : np.array([0,0,0,0,0,0,0,0,1,0,0,0]),\n",
    "    'The_Eiffel_Tower' : np.array([0,0,0,0,0,0,0,0,0,1,0,0]),\n",
    "    'tree' : np.array([0,0,0,0,0,0,0,0,0,0,1,0]),\n",
    "    'umbrella' : np.array([0,0,0,0,0,0,0,0,0,0,0,1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def get_max_len(strokes):\n",
    "    \"\"\"Return the maximum length of an array of strokes.\"\"\"\n",
    "    max_len = 0\n",
    "    for stroke in strokes:\n",
    "        ml = len(stroke)\n",
    "        if ml > max_len:\n",
    "            max_len = ml\n",
    "    return max_len\n",
    "\n",
    "def to_tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        pass\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    else:\n",
    "        raise Exception('input must be a tensor or ndarray.')\n",
    "    return x.float()\n",
    "\n",
    "def init_orthogonal_(weight, hsize):\n",
    "    assert weight.size(0) == 4*hsize\n",
    "    for i in range(4):\n",
    "        nn.init.orthogonal_(weight[i*hsize:(i+1)*hsize])\n",
    "\n",
    "def load_strokes(data_dir):\n",
    "    \n",
    "    \"\"\"Loads the .npz file, \n",
    "    and splits the set into train/valid/test.\"\"\"\n",
    "\n",
    "    train_strokes = None\n",
    "    valid_strokes = None\n",
    "    test_strokes = None\n",
    "    \n",
    "    for label in labels_list:\n",
    "        \n",
    "        data = np.load(os.path.join(data_dir, f'{str(label)}.npz'), \n",
    "                       encoding='latin1', allow_pickle=True)\n",
    "\n",
    "        if train_strokes is None:\n",
    "            train_strokes = data['train']\n",
    "            valid_strokes = data['valid']\n",
    "            test_strokes = data['test']\n",
    "            train_label = np.broadcast_to(labels_dict[label], (len(data['train']), len(labels_dict[label])))\n",
    "            valid_label = np.broadcast_to(labels_dict[label], (len(data['valid']), len(labels_dict[label])))\n",
    "            test_label = np.broadcast_to(labels_dict[label], (len(data['test']), len(labels_dict[label])))\n",
    "        else:\n",
    "            train_strokes = np.concatenate((train_strokes, data['train']))\n",
    "            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
    "            test_strokes = np.concatenate((test_strokes, data['test']))\n",
    "            train_label = np.concatenate((train_label, np.broadcast_to(labels_dict[label], (len(data['train']), len(labels_dict[label])))))\n",
    "            valid_label = np.concatenate((valid_label, np.broadcast_to(labels_dict[label], (len(data['valid']), len(labels_dict[label])))))\n",
    "            test_label = np.concatenate((test_label, np.broadcast_to(labels_dict[label], (len(data['test']), len(labels_dict[label])))))              \n",
    "        \n",
    "    all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n",
    "    \n",
    "    num_points = 0\n",
    "    for stroke in all_strokes:\n",
    "        num_points += len(stroke)\n",
    "    avg_len = num_points / len(all_strokes)\n",
    "    \n",
    "    print('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n",
    "        len(all_strokes), len(train_strokes), len(valid_strokes),\n",
    "        len(test_strokes), int(avg_len)))\n",
    "    \n",
    "    max_seq_len = get_max_len(all_strokes)\n",
    "    \n",
    "    return train_strokes, valid_strokes, test_strokes, train_label, valid_label, test_label, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, strokes, labels, scale_factor=None, max_len=250, limit=1000):\n",
    "        strokes = [to_tensor(stk) for stk in strokes]\n",
    "        self.labels = [to_tensor(lbl) for lbl in labels]\n",
    "        self.max_len = max_len\n",
    "        self.limit = limit\n",
    "        self.preprocess(strokes) # list of drawings in stroke-3 format, sorted by size\n",
    "        self.normalize(scale_factor)\n",
    "        \n",
    "\n",
    "    def preprocess(self, strokes):\n",
    "        \"\"\"Remove entries from strokes having > max_len points.\n",
    "        Clamp x-y values to (-limit, limit)\n",
    "        \"\"\"\n",
    "        raw_data = []\n",
    "        seq_len = []\n",
    "        count_data = 0\n",
    "        for i in range(len(strokes)):\n",
    "            data = strokes[i]\n",
    "            if len(data) <= (self.max_len):\n",
    "                count_data += 1\n",
    "                data = data.clamp(-self.limit, self.limit)\n",
    "                raw_data.append(data)\n",
    "                seq_len.append(len(data))\n",
    "        self.sort_idx = np.argsort(seq_len)\n",
    "        self.strokes = [raw_data[ix] for ix in self.sort_idx]\n",
    "        print(\"total drawings <= max_seq_len is %d\" % count_data)\n",
    "\n",
    "    def calculate_normalizing_scale_factor(self):\n",
    "        \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
    "        strokes = [elt for elt in self.strokes if len(elt) <= self.max_len]\n",
    "        data = torch.cat(strokes)\n",
    "        return data[:,:2].std()\n",
    "\n",
    "    def normalize(self, scale_factor=None):\n",
    "        \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
    "        if scale_factor is None:\n",
    "            scale_factor = self.calculate_normalizing_scale_factor()\n",
    "        self.scale_factor = scale_factor\n",
    "        for i in range(len(self.strokes)):\n",
    "            self.strokes[i][:,:2] /= self.scale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strokes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.strokes[idx]\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─LSTM: 1-1                              24,188,928\n",
      "├─Linear: 1-2                            6,156\n",
      "=================================================================\n",
      "Total params: 24,195,084\n",
      "Trainable params: 24,195,084\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "class QDRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(QDRNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, dropout=0.2, bidirectional=True)\n",
    "        self.classifier = nn.Linear(2*hidden_dim, 12)\n",
    "       \n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, length):\n",
    "        data = rnn_utils.pack_padded_sequence(data, length.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, (_, _) = self.rnn(data) # [2,batch,hid]\n",
    "        x, _ = rnn_utils.pad_packed_sequence(x, batch_first=True)\n",
    "        # x = x.permute(1,0,2).flatten(1).contiguous() # [batch,2*hid]\n",
    "        output = self.classifier(x)\n",
    "        # output = F.softmax(output)\n",
    "        return output\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = QDRNN(3, 128, 8).to(device)\n",
    "print(summary(model, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset combined: 900000 (840000/30000/30000), avg len 63\n"
     ]
    }
   ],
   "source": [
    "train_strokes, valid_strokes, test_strokes, train_label, valid_label, test_label, max_seq_len = load_strokes(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = QuickDrawDataset(train_strokes, train_label, max_len=max_seq_len)\n",
    "validset = QuickDrawDataset(valid_strokes, valid_label, max_len=max_seq_len)\n",
    "\n",
    "def collate_fn(dataset):\n",
    "    train_data = list(list(zip(*dataset))[0])\n",
    "    labels = list(zip(*dataset))[1]\n",
    "    target = torch.Tensor([np.array(l) for l in labels])\n",
    "    train_data.sort(key=lambda data: len(data), reverse=True)\n",
    "    data_length = [len(data) for data in train_data]\n",
    "    data_length = torch.Tensor(data_length)\n",
    "    train_data = rnn_utils.pad_sequence(train_data, batch_first=True, padding_value=0)\n",
    "    return train_data, target, data_length\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=128, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validset, batch_size=128, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "\n",
    "num_epoch = 10\n",
    "device = 'cuda'\n",
    "\n",
    "model = QDRNN(3, 128, 8).to(device)\n",
    "\n",
    "grad_clip = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    with tqdm(total=len(train_loader.dataset)) as progress_bar:\n",
    "        \n",
    "        for idx, (data, target, length) in enumerate(train_loader):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            length = length.to(device, non_blocking=True)\n",
    "            target = target.to(device, dtype=torch.long, non_blocking=True)\n",
    "            # training step\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, length)\n",
    "            loss = criterion(output, target)\n",
    "#             # reshape output to [Batch, NumClass] and target accordingly\n",
    "#             reshaped_output = output[:, -1, :].squeeze()\n",
    "#             reshaped_target = torch.argmax(target, dim=1)\n",
    "#             loss = criterion(reshaped_output, reshaped_target)\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # update loss meter and progbar\n",
    "            loss_meter.update(loss.item(), data.size(0))\n",
    "            progress_bar.set_postfix(loss=loss_meter)\n",
    "            progress_bar.update(data.size(0))\n",
    "        \n",
    "    train_loss = loss_meter.avg\n",
    "            \n",
    "    print(f'Epoch {epoch} | L: {train_loss:.7f}')\n",
    "\n",
    "            \n",
    "    model.eval()\n",
    "    loss_meter = AverageMeter()\n",
    "    with tqdm(total=len(valid_loader.dataset)) as progress_bar:\n",
    "        \n",
    "        for idx, (data, target, length) in enumerate(valid_loader):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            length = length.to(device, non_blocking=True)\n",
    "            target = target.to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "            output = model(data, length)\n",
    "            loss = criterion(output, target)\n",
    "#             # reshape output to [Batch, NumClass] and target accordingly\n",
    "#             reshaped_output = output[:, -1, :].squeeze()\n",
    "#             reshaped_target = torch.argmax(target, dim=1)\n",
    "#             loss = criterion(reshaped_output, reshaped_target)\n",
    "\n",
    "            loss_meter.update(loss.item(), data.size(0))\n",
    "\n",
    "    val_loss = loss_meter.avg\n",
    "        \n",
    "    print(f'Epoch {epoch} | L: {val_loss:.7f}\\n')\n",
    "    \n",
    "    print('Epoch %0.2i, Train Loss: %0.5f, Valid Loss: %0.5f' %\n",
    "              (epoch+1, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/LSTM_[Batch,Class]-e{num_epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and get test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(epoch: int, device: torch.device = 'cuda'):\n",
    "    \n",
    "    model = QDRNN(3, 128, 8).to(device)\n",
    "    model.load_state_dict(torch.load(f'models/LSTM_[Batch,Class]-e{epoch}_reshaped_output.pth'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total drawings <= max_seq_len is 30000\n"
     ]
    }
   ],
   "source": [
    "testset = QuickDrawDataset(test_strokes, test_label, max_len=max_seq_len)\n",
    "\n",
    "def collate_fn(dataset):\n",
    "    train_data = list(list(zip(*dataset))[0])\n",
    "    labels = list(zip(*dataset))[1]\n",
    "    target = torch.Tensor([np.array(l) for l in labels])\n",
    "    train_data.sort(key=lambda data: len(data), reverse=True)\n",
    "    data_length = [len(data) for data in train_data]\n",
    "    data_length = torch.Tensor(data_length)\n",
    "    train_data = rnn_utils.pad_sequence(train_data, batch_first=True, padding_value=0)\n",
    "    return train_data, target, data_length\n",
    "\n",
    "test_loader = DataLoader(testset, batch_size=128, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = load_model(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:2.4844, test_acc:0.91667\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.eval()\n",
    "loss_meter = AverageMeter()\n",
    "test_acc = 0.0\n",
    "for idx, (data, target, length) in enumerate(test_loader):\n",
    "    data = data.to(device, non_blocking=True)\n",
    "    length = length.to(device, non_blocking=True)\n",
    "    target = target.to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "    output = model(data, length)\n",
    "#     loss = criterion(output, target)\n",
    "    # reshape output to [Batch, NumClass] and target accordingly\n",
    "    reshaped_output = output[:, -1, :].squeeze()\n",
    "    reshaped_target = torch.argmax(target, dim=1)\n",
    "    loss = criterion(reshaped_output, reshaped_target)\n",
    "    \n",
    "    loss_meter.update(loss.item(), data.size(0))\n",
    "    \n",
    "    outputs = reshaped_output > 0.0\n",
    "    test_acc += (outputs == target).float().mean()\n",
    "    \n",
    "test_loss = loss_meter.avg\n",
    "\n",
    "print(f'test loss:{test_loss:.5}, test_acc:{test_acc/(idx+1):.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
